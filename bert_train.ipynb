{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNU2qqTxmHPrQgJD/VnPFjY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JasonLaux/nlp/blob/main/bert_train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LVPoZ4LcFj1W"
      },
      "source": [
        "The configuration on the model uses 10-bert.ipython for reference...\n",
        "This file is used to train the classification model and save it for the analysis in Task 2. The trained model is named as sstcls.pth. The file initially runs on the Google Colab."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1d6Ig15COmUT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0cfc79f4-228b-4d23-8e99-8617a885b278"
      },
      "source": [
        "from google.colab import drive\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import BertModel, BertTokenizer\n",
        "import torch.optim as optim\n",
        "import time\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import json\n",
        "from collections import OrderedDict, deque\n",
        "import datetime\n",
        "import gc\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dcRNBLUUUptt"
      },
      "source": [
        "!pip install torch torchvision transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQGyMorkTWSu"
      },
      "source": [
        "# The path './gdrive/MyDrive/nlp' refers to the location on Google Drive. \n",
        "DATA_TRAIN_PATH = './gdrive/MyDrive/nlp/train.data.jsonl'\n",
        "LABEL_TRAIN_PATH = './gdrive/MyDrive/nlp/train.label.json'\n",
        "DATA_DEV_PATH = './gdrive/MyDrive/nlp/dev.data.jsonl'\n",
        "LABEL_DEV_PATH = './gdrive/MyDrive/nlp/dev.label.json'\n",
        "DATA_TEST_PATH = './gdrive/MyDrive/nlp/test.data.jsonl'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h0ViFcyzFVyU"
      },
      "source": [
        "# # Use this path configuration instead if running locally...\n",
        "# DATA_TRAIN_PATH = './data/train.data.jsonl'\n",
        "# LABEL_TRAIN_PATH = './data/train.label.json'\n",
        "# DATA_DEV_PATH = './data/dev.data.jsonl'\n",
        "# LABEL_DEV_PATH = './data/dev.label.json'\n",
        "# DATA_TEST_PATH = './data/test.data.jsonl'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "anNWiP-bQAe5"
      },
      "source": [
        "'''\n",
        "Define the graph struction to sort replies in a reasonable order. The mechanism \n",
        "is explained in the report.\n",
        "'''\n",
        "class DAG(object):\n",
        "    \"\"\" Directed acyclic graph implementation. \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\" Construct a new DAG with no nodes or edges. \"\"\"\n",
        "        self.node_depth = []\n",
        "        self.graph = OrderedDict()\n",
        "\n",
        "    def add_node(self, node: tuple):\n",
        "        \"\"\" Add a node if it does not exist yet, or error out. \"\"\"\n",
        "        if node in self.graph:\n",
        "            raise KeyError('node %s already exists' % node.index)\n",
        "        self.graph[node] = set()\n",
        "\n",
        "    def add_edge(self, start_node: tuple, end_node: tuple):\n",
        "        \"\"\" Add an edge (dependency) between the specified nodes. \"\"\"\n",
        "\n",
        "        if start_node not in self.graph or end_node not in self.graph:\n",
        "            raise KeyError(\"Node is not existed in the graph.\")\n",
        "\n",
        "        self.graph[start_node].add(end_node)\n",
        "\n",
        "    def sort_children(self):\n",
        "      \"\"\" Sort children node by time in ascending order. \"\"\"\n",
        "        for key in self.graph:\n",
        "            self.graph[key] = sorted(self.graph[key], key=lambda item: item[1])\n",
        "\n",
        "    def topological_sort(self):\n",
        "        \"\"\" Returns a topological ordering of the DAG.\n",
        "        Raises an error if this is not possible (graph is not valid).\n",
        "        \"\"\"\n",
        "\n",
        "        in_degree = {}\n",
        "        for u in self.graph:\n",
        "            in_degree[u] = 0\n",
        "\n",
        "        for u in self.graph:\n",
        "            for v in self.graph[u]:\n",
        "                in_degree[v] += 1\n",
        "\n",
        "        queue = deque()\n",
        "        for u in in_degree:\n",
        "            if in_degree[u] == 0:\n",
        "                queue.appendleft(u)\n",
        "\n",
        "        l = []\n",
        "        while queue:\n",
        "            u = queue.pop()\n",
        "            l.append(u)\n",
        "            for v in self.graph[u]:\n",
        "                in_degree[v] -= 1\n",
        "                if in_degree[v] == 0:\n",
        "                    queue.appendleft(v)\n",
        "\n",
        "        if len(l) == len(self.graph):\n",
        "            return l\n",
        "        else:\n",
        "            raise ValueError('graph is not acyclic')\n",
        "\n",
        "def create_graph(items):\n",
        "    graph = DAG()\n",
        "    root_time = \"\"\n",
        "    idStr_idx = {}\n",
        "\n",
        "    for idx, item in enumerate(items):\n",
        "        if idx == 0:\n",
        "            root_time = item[\"created_at\"]\n",
        "        commit_time = item[\"created_at\"]\n",
        "        idStr_idx.update({item[\"id_str\"]: idx})  # Assuming every tweet is unique\n",
        "        graph.add_node((idx, calc_time_diff(root_time, commit_time)))\n",
        "\n",
        "    keys = list(graph.graph.copy())\n",
        "\n",
        "    for idx, item in enumerate(items):\n",
        "        parent_idx = idStr_idx.get(str(item[\"in_reply_to_status_id_str\"]))\n",
        "        if parent_idx is not None:\n",
        "            graph.add_edge(keys[parent_idx], keys[idx])\n",
        "\n",
        "    graph.sort_children()\n",
        "\n",
        "    return graph\n",
        "\n",
        "\n",
        "def calc_time_diff(start_time: str, end_time: str):\n",
        "    start_time_formatted = datetime.datetime.strptime(start_time, \"%a %b %d %H:%M:%S %z %Y\")\n",
        "    end_time_formatted = datetime.datetime.strptime(end_time, \"%a %b %d %H:%M:%S %z %Y\")\n",
        "    return (end_time_formatted - start_time_formatted).total_seconds()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4QulgoC-QOZO"
      },
      "source": [
        "class TweetDataset(Dataset):\n",
        "\n",
        "    def __init__(self, fn_data, fn_label=None, maxlen=256):\n",
        "        # Store the contents of the file in a pandas dataframe\n",
        "        self.data = open(fn_data, encoding=\"utf-8\").readlines()\n",
        "        if fn_label is not None:\n",
        "            self.label_dict = json.load(open(fn_label, encoding=\"utf-8\"))\n",
        "        else:\n",
        "            self.label_dict = None\n",
        "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "        # Initialize the BERT tokenizer\n",
        "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "        self.maxlen = maxlen  # the max length of the sentence in the corpus\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        items = json.loads(self.data[index])\n",
        "        id_str = items[0][\"id_str\"]  # Tweet index\n",
        "        idx_list = [pair[0] for pair in create_graph(items).topological_sort()]\n",
        "        tokens_list = []\n",
        "        for idx in idx_list:\n",
        "            username = items[idx][\"user\"][\"name\"]\n",
        "            text = items[idx][\"text\"]\n",
        "            current_sentence = username + \":\" + text\n",
        "            tokens_list.append(self.tokenizer.tokenize(current_sentence))\n",
        "\n",
        "        tokens_concat = ['[CLS]'] + [token for item in tokens_list for token in item] + ['[SEP]']\n",
        "        if len(tokens_concat) < self.maxlen:\n",
        "            padded_tokens = tokens_concat + ['[PAD]' for _ in range(self.maxlen - len(tokens_concat))]\n",
        "        else:\n",
        "            padded_tokens = tokens_concat[:self.maxlen - 1] + ['[SEP]']\n",
        "\n",
        "        tokens_ids = self.tokenizer.convert_tokens_to_ids(padded_tokens)\n",
        "        attn_mask = [1 if token != '[PAD]' else 0 for token in padded_tokens]\n",
        "        tokens_ids_tensor = torch.tensor(tokens_ids)\n",
        "        attn_masks_tensor = torch.tensor(attn_mask)\n",
        "        label_idx = torch.tensor(-1)\n",
        "        if self.label_dict:\n",
        "            label = self.label_dict[id_str]  # Tweet label\n",
        "            if label == \"non-rumour\":\n",
        "                label_idx = torch.tensor(1)\n",
        "            else:\n",
        "                label_idx = torch.tensor(0)\n",
        "        return tokens_ids_tensor, attn_masks_tensor, label_idx, id_str"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BbxrXgpJQPmR"
      },
      "source": [
        "class RumourClassifier(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(RumourClassifier, self).__init__()\n",
        "        # Instantiating BERT model object\n",
        "        self.bert_layer = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "        # Classification layer\n",
        "        # input dimension is 768 because [CLS] embedding has a dimension of 768\n",
        "        # output dimension is 1 because we're working with a binary classification problem\n",
        "        self.cls_layer = nn.Linear(768, 1)\n",
        "\n",
        "    def forward(self, tokens_ids, attn_masks):\n",
        "        '''\n",
        "        Inputs:\n",
        "            -seq : Tensor of shape [B, T] containing token ids of sequences\n",
        "            -attn_masks : Tensor of shape [B, T] containing attention masks to be used to avoid contibution of PAD tokens\n",
        "        '''\n",
        "\n",
        "        # Feeding the input to BERT model to obtain contextualized representations\n",
        "        outputs = self.bert_layer(input_ids=tokens_ids, attention_mask=attn_masks)\n",
        "        cont_reps = outputs.last_hidden_state\n",
        "\n",
        "        # Obtaining the representation of [CLS] head (the first token)\n",
        "        cls_rep = cont_reps[:, 0]\n",
        "\n",
        "        # Feeding cls_rep to the classifier layer\n",
        "        logits = self.cls_layer(cls_rep)\n",
        "\n",
        "        return logits\n",
        "\n",
        "\n",
        "def get_accuracy_from_logits(logits, labels):\n",
        "    probs = torch.sigmoid(logits.unsqueeze(-1))\n",
        "    soft_probs = (probs > 0.5).long()\n",
        "    acc = (soft_probs.squeeze() == labels).float().mean()\n",
        "    return acc\n",
        "\n",
        "\n",
        "def evaluate(net, criterion, dataloader, gpu):\n",
        "    net.eval()\n",
        "\n",
        "    mean_acc, mean_loss = 0, 0\n",
        "    count = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for seq, attn_masks, labels, _ in dataloader:\n",
        "            seq, attn_masks, labels = seq.cuda(gpu), attn_masks.cuda(gpu), labels.cuda(gpu)\n",
        "            logits = net(seq, attn_masks)\n",
        "            mean_loss += criterion(logits.squeeze(-1), labels.float()).item()\n",
        "            mean_acc += get_accuracy_from_logits(logits, labels)\n",
        "            count += 1\n",
        "    \n",
        "    if count == 0:\n",
        "      raise KeyError(\"Dataloader is loaded incorrectly!\")\n",
        "    else:\n",
        "      return mean_acc / count, mean_loss / count\n",
        "\n",
        "\n",
        "def train(net, criterion, opti, train_loader, dev_loader, max_eps, gpu):\n",
        "    best_acc = 0\n",
        "    st = time.time()\n",
        "    for ep in range(max_eps):\n",
        "\n",
        "        for it, (seq, attn_masks, labels, _) in enumerate(train_loader):\n",
        "            # Clear gradients\n",
        "            opti.zero_grad()\n",
        "            # Converting these to cuda tensors\n",
        "            seq, attn_masks, labels = seq.cuda(gpu), attn_masks.cuda(gpu), labels.cuda(gpu)\n",
        "\n",
        "            # Obtaining the logits from the model\n",
        "            logits = net(seq, attn_masks)\n",
        "\n",
        "            # Computing loss\n",
        "            loss = criterion(logits.squeeze(-1), labels.float())\n",
        "\n",
        "            # Backpropagating the gradients\n",
        "            loss.backward()\n",
        "\n",
        "            # Optimization step\n",
        "            opti.step()\n",
        "\n",
        "            if it % 100 == 0:\n",
        "                acc = get_accuracy_from_logits(logits, labels)\n",
        "                print(\"Iteration {} of epoch {} complete. Loss: {}; Accuracy: {}; Time taken (s): {}\"\n",
        "                      .format(it, ep, loss.item(), acc, (time.time() - st)))\n",
        "                st = time.time()\n",
        "\n",
        "        dev_acc, dev_loss = evaluate(net, criterion, dev_loader, gpu)\n",
        "        print(\"Epoch {} complete! Development Accuracy: {}; Development Loss: {}\".format(ep, dev_acc, dev_loss))\n",
        "        if dev_acc > best_acc:\n",
        "            print(\"Best development accuracy improved from {} to {}, saving model...\".format(best_acc, dev_acc))\n",
        "            best_acc = dev_acc\n",
        "            torch.save(net.state_dict(), 'sstcls.pth'.format(ep))\n",
        "    return net"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_4arGlWlb2t"
      },
      "source": [
        "def predict(net, dataloader, gpu):\n",
        "    net.eval()\n",
        "    dict_pred = {}\n",
        "    with torch.no_grad():\n",
        "        for seq, attn_masks, _, id_str in dataloader:\n",
        "            seq, attn_masks = seq.cuda(gpu), attn_masks.cuda(gpu)\n",
        "            logits = net(seq, attn_masks)\n",
        "            probs = torch.sigmoid(logits.unsqueeze(-1))\n",
        "            y_pred = (probs > 0.5).long().squeeze()\n",
        "            if y_pred == torch.tensor(0):\n",
        "                label = \"rumour\"\n",
        "            else:\n",
        "                label = \"non-rumour\"\n",
        "            dict_pred.update({id_str[0]: label})\n",
        "    with open(\"test_label.json\", \"w+\") as f:\n",
        "        json.dump(dict_pred, f)\n",
        "    return dict_pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BjKJ36JoQVE2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8c5aff5-ea0d-4c58-9896-88f11dd5dba8"
      },
      "source": [
        "gpu = 0\n",
        "print(\"Creating the classifier, initialised with pretrained BERT-BASE parameters...\")\n",
        "net = RumourClassifier()\n",
        "net.cuda(gpu)  # Enable gpu support for the model\n",
        "print(\"Done creating the classifier.\")\n",
        "\n",
        "# Define loss function based on binary cross-entropy.\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "opti = optim.Adam(net.parameters(), lr=2e-5)\n",
        "num_epoch = 3 \n",
        "\n",
        "# The maxlen is limited to 256 due to the out-of-memory issue\n",
        "train_dataset = TweetDataset(DATA_TRAIN_PATH, LABEL_TRAIN_PATH, maxlen=256)\n",
        "dev_dataset = TweetDataset(DATA_DEV_PATH, LABEL_DEV_PATH, maxlen=256)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=10)\n",
        "dev_dataloader = DataLoader(dev_dataset, batch_size=10)\n",
        "test_dataset = TweetDataset(DATA_TEST_PATH, maxlen=256)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=1)\n",
        "\n",
        "\n",
        "net_trained = train(net, criterion, opti, train_dataloader, dev_dataloader, num_epoch, gpu)\n",
        "print(predict(net_trained, test_dataloader, gpu))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Creating the classifier, initialised with pretrained BERT-BASE parameters...\n",
            "Done creating the classifier.\n",
            "Iteration 0 of epoch 0 complete. Loss: 0.9059423804283142; Accuracy: 0.30000001192092896; Time taken (s): 1.0903050899505615\n",
            "Iteration 100 of epoch 0 complete. Loss: 0.7962539196014404; Accuracy: 0.4000000059604645; Time taken (s): 93.4114236831665\n",
            "Iteration 200 of epoch 0 complete. Loss: 0.292822927236557; Accuracy: 0.9000000357627869; Time taken (s): 93.8758008480072\n",
            "Iteration 300 of epoch 0 complete. Loss: 0.5758542418479919; Accuracy: 0.800000011920929; Time taken (s): 94.28265810012817\n",
            "Iteration 400 of epoch 0 complete. Loss: 0.6749157905578613; Accuracy: 0.800000011920929; Time taken (s): 93.87975454330444\n",
            "Epoch 0 complete! Development Accuracy: 0.8758621215820312; Development Loss: 0.29929105385110294\n",
            "Best development accuracy improved from 0 to 0.8758621215820312, saving model...\n",
            "Iteration 0 of epoch 1 complete. Loss: 0.38689208030700684; Accuracy: 0.800000011920929; Time taken (s): 85.73731637001038\n",
            "Iteration 100 of epoch 1 complete. Loss: 0.5454830527305603; Accuracy: 0.699999988079071; Time taken (s): 93.47332310676575\n",
            "Iteration 200 of epoch 1 complete. Loss: 0.1455778330564499; Accuracy: 0.9000000357627869; Time taken (s): 93.7924919128418\n",
            "Iteration 300 of epoch 1 complete. Loss: 0.31349167227745056; Accuracy: 0.9000000357627869; Time taken (s): 94.41562247276306\n",
            "Iteration 400 of epoch 1 complete. Loss: 0.5556401610374451; Accuracy: 0.699999988079071; Time taken (s): 94.19096899032593\n",
            "Epoch 1 complete! Development Accuracy: 0.8741379976272583; Development Loss: 0.3316180411533549\n",
            "Iteration 0 of epoch 2 complete. Loss: 0.19330331683158875; Accuracy: 1.0; Time taken (s): 84.91258955001831\n",
            "Iteration 100 of epoch 2 complete. Loss: 0.06666790693998337; Accuracy: 1.0; Time taken (s): 93.6012830734253\n",
            "Iteration 200 of epoch 2 complete. Loss: 0.004096332937479019; Accuracy: 1.0; Time taken (s): 94.51361799240112\n",
            "Iteration 300 of epoch 2 complete. Loss: 0.2635675370693207; Accuracy: 0.9000000357627869; Time taken (s): 94.62642455101013\n",
            "Iteration 400 of epoch 2 complete. Loss: 0.14015866816043854; Accuracy: 0.9000000357627869; Time taken (s): 94.15782928466797\n",
            "Epoch 2 complete! Development Accuracy: 0.882758617401123; Development Loss: 0.3782129245569351\n",
            "Best development accuracy improved from 0.8758621215820312 to 0.882758617401123, saving model...\n",
            "{'544382249178001408': 'rumour', '525027317551079424': 'rumour', '544273220128739329': 'rumour', '499571799764770816': 'non-rumour', '552844104418091008': 'non-rumour', '524977651476623360': 'rumour', '544514988078280704': 'non-rumour', '524928863714168832': 'rumour', '544390718253699072': 'non-rumour', '580322349569994752': 'non-rumour', '544475905926524928': 'non-rumour', '544389986809036800': 'non-rumour', '498530293116968960': 'non-rumour', '498293625420148736': 'non-rumour', '552831230735962113': 'non-rumour', '553589469849583616': 'non-rumour', '544415816851021824': 'non-rumour', '552850116324130816': 'non-rumour', '544318036715782144': 'rumour', '524974318087061504': 'rumour', '553592195786506240': 'rumour', '524959778125385728': 'non-rumour', '553502311872733184': 'non-rumour', '499698366402789376': 'non-rumour', '525032872647065600': 'rumour', '553110609513177088': 'rumour', '544267656597995521': 'rumour', '552845393541988352': 'non-rumour', '500422971320963072': 'non-rumour', '553197197618339841': 'non-rumour', '500351760402493440': 'non-rumour', '553147341470203906': 'non-rumour', '524939821815721984': 'non-rumour', '544376469279875072': 'non-rumour', '544438309267578880': 'non-rumour', '544453205682556928': 'non-rumour', '552821573946982401': 'non-rumour', '552817675408523264': 'non-rumour', '544336432609775618': 'non-rumour', '552825473160736768': 'non-rumour', '544429350376861696': 'non-rumour', '552846544320618496': 'non-rumour', '524950203476869120': 'non-rumour', '500371231183745025': 'rumour', '500234390723915776': 'non-rumour', '544515971637968898': 'non-rumour', '544382795099799553': 'non-rumour', '553194557316542465': 'non-rumour', '552837644959621120': 'non-rumour', '553535829529100288': 'rumour', '544465862942797825': 'non-rumour', '581183387848859648': 'rumour', '544487579048222721': 'non-rumour', '499531968229683200': 'non-rumour', '498294764718915586': 'non-rumour', '500298315368390657': 'rumour', '544513510450155520': 'non-rumour', '580329219646988289': 'non-rumour', '500382853147164672': 'non-rumour', '544353502689251329': 'rumour', '552814137857036289': 'non-rumour', '524956415589621760': 'non-rumour', '553485654030446592': 'non-rumour', '552840606074953728': 'non-rumour', '552833527444566016': 'non-rumour', '499541439962185729': 'non-rumour', '552824885484224513': 'non-rumour', '499586890837016579': 'non-rumour', '552821810572439553': 'non-rumour', '544310222547087361': 'non-rumour', '552790565642067968': 'non-rumour', '553103772852191233': 'rumour', '500072455931588611': 'non-rumour', '580323877429112832': 'non-rumour', '524969501704855552': 'rumour', '524979702940045313': 'non-rumour', '499427218171248641': 'rumour', '524958409381384192': 'non-rumour', '552848590373392384': 'non-rumour', '524967510563889152': 'non-rumour', '580333904126742528': 'rumour', '544293021995393024': 'non-rumour', '553502446769942528': 'non-rumour', '544326461952241664': 'rumour', '553226143307161600': 'non-rumour', '524936793633083394': 'non-rumour', '552848704819183616': 'non-rumour', '499691593985957888': 'non-rumour', '499432169836531712': 'non-rumour', '580347360959717376': 'rumour', '544426421331169280': 'non-rumour', '553227441712668672': 'non-rumour', '544388658334146561': 'rumour', '552831761957523456': 'non-rumour', '544374018661687296': 'rumour', '500297243371798528': 'non-rumour', '553208762291339265': 'non-rumour', '500320286341099523': 'non-rumour', '552784526955806720': 'rumour', '552978099357237248': 'non-rumour', '500284494201757696': 'rumour', '525021083892994049': 'rumour', '499637476315910146': 'non-rumour', '524962199543230464': 'rumour', '499606771246567424': 'non-rumour', '552833795142209536': 'non-rumour', '552824126659104771': 'non-rumour', '544278985249550337': 'rumour', '553475269873643520': 'rumour', '525032458417610752': 'non-rumour', '499357077429817344': 'non-rumour', '498497371899449344': 'rumour', '525023858831523841': 'rumour', '544285245600989184': 'rumour', '500364250461003777': 'rumour', '553491517462315008': 'non-rumour', '552837499937378305': 'non-rumour', '553215658784456705': 'non-rumour', '553145236268916736': 'non-rumour', '499596695705894912': 'non-rumour', '544466920880492544': 'rumour', '500277382461530112': 'rumour', '553216874130206721': 'non-rumour', '552810032765497345': 'non-rumour', '544313370892324864': 'non-rumour', '500422447623966721': 'non-rumour', '544341194822520832': 'non-rumour', '552806056154644480': 'non-rumour', '500300813835988992': 'rumour', '544462918465503233': 'non-rumour', '552996335319007233': 'rumour', '500377192841609217': 'non-rumour', '553536603998945280': 'rumour', '544284455909998592': 'rumour', '524978196589334529': 'non-rumour', '544427938372792321': 'rumour', '552799640488517632': 'rumour', '552826150141960192': 'non-rumour', '500296369224310784': 'non-rumour', '553124783043649537': 'non-rumour', '499464258183114754': 'non-rumour', '553588704913416192': 'non-rumour', '581317224096387072': 'rumour', '580364973664309248': 'rumour', '499692788993511424': 'non-rumour', '544478625383608320': 'non-rumour', '553472225798795264': 'rumour', '553591049541914624': 'non-rumour', '524943012602716160': 'non-rumour', '544282355989753857': 'rumour', '525020733622464512': 'rumour', '553157857215262721': 'non-rumour', '544410050572582912': 'non-rumour', '544329393737170944': 'rumour', '525026715123601408': 'rumour', '552834732325879809': 'rumour', '552822314157768704': 'non-rumour', '552804839886184448': 'rumour', '544274544174071809': 'non-rumour', '553576010898497536': 'rumour', '544410900854091776': 'non-rumour', '580326762673901568': 'rumour', '552803792618799106': 'non-rumour', '552823427862908928': 'non-rumour', '553506985413713920': 'non-rumour', '500311883669585920': 'non-rumour', '544351885298130945': 'rumour', '544469609433563136': 'non-rumour', '500307001629745152': 'rumour', '524927288077746176': 'rumour', '552825211654246401': 'non-rumour', '544319905076903936': 'non-rumour', '524964630221705216': 'non-rumour', '553218599390040065': 'non-rumour', '544391412075069440': 'non-rumour', '499364002318020608': 'non-rumour', '500343812863311872': 'rumour', '553511279235964928': 'non-rumour', '552847959864274944': 'non-rumour', '553165824979386368': 'non-rumour', '499437429690888192': 'non-rumour', '500413012822745090': 'rumour', '580377478184755201': 'rumour', '500315536937324544': 'non-rumour', '525038096086499328': 'rumour', '580364656608428032': 'rumour', '580321156508577792': 'rumour', '524961055211286528': 'non-rumour', '580354367796166656': 'non-rumour', '544427255439425536': 'non-rumour', '553175336926449664': 'non-rumour', '553099479381848064': 'non-rumour', '524958498770395137': 'non-rumour', '544519635916169218': 'rumour', '499440386767859712': 'non-rumour', '553542282100871168': 'non-rumour', '553584782987497474': 'non-rumour', '552815122897727488': 'non-rumour', '553099050409807872': 'non-rumour', '553167429665587200': 'rumour', '525050193360613376': 'non-rumour', '500234112054341632': 'non-rumour', '500332042648055808': 'rumour', '500313811195158528': 'non-rumour', '553556643456507904': 'rumour', '553192366098898945': 'non-rumour', '524962748237889536': 'rumour', '524959090125320193': 'non-rumour', '498300256678076417': 'non-rumour', '553512076304728064': 'non-rumour', '544414686935285760': 'non-rumour', '500287538578587648': 'rumour', '498276354434277376': 'non-rumour', '524949073154301952': 'rumour', '525021050627973120': 'non-rumour', '524977830124617728': 'non-rumour', '544401106617786370': 'non-rumour', '552824705703763969': 'non-rumour', '524944006577668096': 'rumour', '581153923987206146': 'rumour', '500359125310922753': 'rumour', '544512078518968320': 'non-rumour', '553535765997969408': 'non-rumour', '580364138020507649': 'rumour', '553491005971132416': 'rumour', '553205115830951936': 'non-rumour', '552844643097120768': 'non-rumour', '544416937178976256': 'non-rumour', '500371149713178625': 'non-rumour', '500278978557804544': 'non-rumour', '553539631149756416': 'non-rumour', '552841049467400192': 'non-rumour', '553474514496278528': 'rumour', '553536887349325825': 'rumour', '552835339325558785': 'non-rumour', '544328894812549121': 'rumour', '553583867664535552': 'non-rumour', '544328059482939392': 'non-rumour', '524965242213007360': 'non-rumour', '524930671220105216': 'rumour', '544379366474403840': 'rumour', '553528503573184514': 'rumour', '524942952733220865': 'rumour', '499360762167848960': 'rumour', '544362279006113792': 'rumour', '499611069565112320': 'non-rumour', '553546292748898304': 'non-rumour', '553588106587545600': 'non-rumour', '499706791396392960': 'non-rumour', '553534934447824896': 'non-rumour', '544277516773380097': 'rumour', '500407038800056321': 'rumour', '552795953301045248': 'non-rumour', '552828400843620353': 'non-rumour', '498272364560257025': 'non-rumour', '544285712653500419': 'rumour', '553107688591089664': 'non-rumour', '552813160110252032': 'non-rumour', '499659628645720064': 'non-rumour', '544390550497947649': 'non-rumour', '524971492820647936': 'non-rumour', '580326222107951104': 'rumour', '553185404657352704': 'non-rumour', '553129811850559488': 'non-rumour', '544309177859203073': 'rumour', '524950455743291392': 'non-rumour', '524955388190662657': 'non-rumour', '500367267901632512': 'non-rumour', '500363126294863876': 'rumour', '552788555584765952': 'non-rumour', '553143519536500738': 'non-rumour', '552807328287064065': 'non-rumour', '498341675476217856': 'rumour', '552825257707335680': 'non-rumour', '524935633463037953': 'rumour', '544341406094213121': 'non-rumour', '544435176479027200': 'non-rumour', '552824320389435392': 'rumour', '580320745974288384': 'rumour', '498251940997136384': 'non-rumour', '553582206019723264': 'non-rumour', '553589372084162560': 'rumour', '580332189155459072': 'rumour', '544268637134393344': 'rumour', '544338731210399744': 'rumour', '553226059836297216': 'non-rumour', '553529796601585664': 'rumour', '524974975623892992': 'non-rumour', '553222727700865025': 'non-rumour', '499435341259223040': 'non-rumour', '553478677359775744': 'non-rumour', '498356729634390016': 'non-rumour', '553531783992868864': 'non-rumour', '499691638424608768': 'non-rumour', '552826580741795840': 'non-rumour', '544434192478519297': 'rumour', '553548404102414337': 'non-rumour', '524943886553468929': 'rumour', '580322026029654018': 'non-rumour', '499438052688023552': 'non-rumour', '499394946416529409': 'non-rumour', '499583349267382273': 'non-rumour', '524961721744900097': 'rumour', '581293685041557504': 'rumour', '553589657695715328': 'non-rumour', '524955083297931264': 'non-rumour', '552981833189969921': 'rumour', '553174338380517376': 'non-rumour', '552830966411321344': 'non-rumour', '553549680538578944': 'rumour', '524952094986350592': 'rumour', '500359356589019136': 'rumour', '498489114384420865': 'non-rumour', '552818988468944897': 'non-rumour', '544361776020586496': 'rumour', '553233130275221504': 'non-rumour', '580882417117958144': 'rumour', '544453251068743681': 'non-rumour', '499376069485821952': 'non-rumour', '524947036228317184': 'non-rumour', '525039002307424256': 'rumour', '499696137708658688': 'non-rumour', '544408991649251329': 'non-rumour', '553176929411825664': 'non-rumour', '544494652418949120': 'non-rumour', '524979881525137409': 'non-rumour', '553503546382249984': 'rumour', '553582801833185280': 'non-rumour', '553550030553243650': 'non-rumour', '524964457936863232': 'non-rumour', '580335545265627136': 'rumour', '500329800725442562': 'non-rumour', '553153190859141120': 'non-rumour', '544520042810200064': 'non-rumour', '499556227874304000': 'non-rumour', '552831978522021890': 'non-rumour', '553587329626292224': 'non-rumour', '544300674448887809': 'rumour', '552981008518500353': 'non-rumour', '580322927679279105': 'rumour', '544269749405097984': 'non-rumour', '500295333486657536': 'rumour', '499349294034743297': 'non-rumour', '553587352086384640': 'non-rumour', '544516478821613569': 'non-rumour', '544500883963518976': 'rumour', '544518803225604096': 'non-rumour', '524941504796962816': 'rumour', '552832584045330432': 'non-rumour', '500298847550472194': 'rumour', '544494058287423489': 'non-rumour', '525070811439579138': 'non-rumour', '544519140552507392': 'rumour', '525001708318240768': 'non-rumour', '552982124274647042': 'non-rumour', '544291966536523776': 'non-rumour', '544414430025371649': 'rumour', '500364914796802048': 'non-rumour', '544301961638457344': 'non-rumour', '500385585807499266': 'non-rumour', '499394221984714752': 'rumour', '580319651399385088': 'rumour', '544432661419155456': 'non-rumour', '580324721381740544': 'rumour', '544318880001560576': 'rumour', '553567184002502656': 'non-rumour', '553580499395174400': 'non-rumour', '500388703114895360': 'non-rumour', '498294367405477888': 'rumour', '525000220371734528': 'non-rumour', '553540824768991233': 'non-rumour', '580320995266936832': 'rumour', '553153740925304832': 'non-rumour', '498267196833808384': 'non-rumour', '544391790615605249': 'non-rumour', '552985855854661633': 'non-rumour', '552806890498191361': 'non-rumour', '552808242276229120': 'non-rumour', '524931830173421568': 'rumour', '524979179235069952': 'non-rumour', '500360632093646848': 'non-rumour', '500070699768496128': 'non-rumour', '544466903364673536': 'non-rumour', '500369097360953345': 'non-rumour', '500072334578159616': 'non-rumour', '525053517531062272': 'non-rumour', '553149163181838336': 'non-rumour', '544353552177455104': 'rumour', '544510827433558016': 'rumour', '499643185581142016': 'non-rumour', '552828658747187201': 'non-rumour', '544397240601563137': 'non-rumour', '524951585231994880': 'rumour', '499703651079045121': 'non-rumour', '544352727971954690': 'rumour', '552843136238899200': 'non-rumour', '580320242020290560': 'rumour', '500274697485824000': 'non-rumour', '500364003734867970': 'non-rumour', '552821030117711872': 'non-rumour', '580334563932549120': 'rumour', '553555848794877952': 'non-rumour', '500266967102914560': 'non-rumour', '552837027616804864': 'non-rumour', '500309381930844160': 'rumour', '500359983301939200': 'rumour', '544492984880754688': 'rumour', '499409455814287360': 'non-rumour', '499642374428307457': 'non-rumour', '524933380929245184': 'rumour', '553187959835725824': 'non-rumour', '552816883012239360': 'non-rumour', '499667557247614977': 'non-rumour', '524941124893700096': 'rumour', '544292129972170752': 'rumour', '544495359800930304': 'non-rumour', '552844436460560384': 'non-rumour', '499706705354448897': 'non-rumour', '499365436816105473': 'non-rumour', '552978586559188992': 'non-rumour', '544369574842347520': 'rumour', '553192641328734208': 'non-rumour', '524947149134774272': 'rumour', '552794995942772736': 'non-rumour', '553587511864598529': 'non-rumour', '552815443959091200': 'non-rumour', '580347361039413248': 'rumour', '524957974226550784': 'non-rumour', '544487291465392128': 'non-rumour', '524965775036387329': 'rumour', '500296080710705152': 'non-rumour', '553159773555023872': 'rumour', '553487579077550082': 'non-rumour', '498294775649665024': 'non-rumour', '524944610985263104': 'non-rumour', '553555389611274240': 'rumour', '544289941996326912': 'non-rumour', '524955243185176576': 'rumour', '544403765575839744': 'non-rumour', '544336537870434304': 'rumour', '524978999534317568': 'non-rumour', '544315472075042818': 'rumour', '500315532382707712': 'non-rumour', '525027651908800512': 'rumour', '544446828654366720': 'rumour', '552851772658561028': 'non-rumour', '524928195011698688': 'non-rumour', '553589285342175232': 'rumour', '544335202923737089': 'non-rumour', '552815293349650433': 'non-rumour', '553581958455123968': 'non-rumour', '525012689568161792': 'non-rumour', '553121732387946496': 'rumour', '552812623658762240': 'non-rumour', '544437798426509312': 'non-rumour', '553484760233017344': 'rumour', '553554585634496512': 'rumour', '552846565950627840': 'non-rumour', '525025329916948481': 'rumour', '553148339160907776': 'non-rumour', '499655603405725696': 'non-rumour', '500316260081889282': 'non-rumour', '553139058827096064': 'non-rumour', '553152208691539968': 'non-rumour', '525034332474601472': 'non-rumour', '498349598197702656': 'rumour', '525047104239333377': 'non-rumour', '553110339236409346': 'rumour', '552814410763603968': 'non-rumour', '552792253601947648': 'non-rumour', '552814770542628865': 'non-rumour', '580330362070540288': 'non-rumour', '580340423044018176': 'non-rumour', '500385877684539393': 'non-rumour', '499665704300191745': 'non-rumour', '499651830951841794': 'non-rumour', '544369653015801856': 'non-rumour', '581307361857392642': 'rumour', '499601956491374594': 'non-rumour', '524924034124107776': 'rumour', '580327960638521344': 'non-rumour', '525002243100401664': 'non-rumour', '525028171549523971': 'rumour', '499530130487017472': 'non-rumour', '498271531386953728': 'non-rumour', '580337440738713600': 'non-rumour', '544477048048082945': 'non-rumour', '524938162900967424': 'rumour', '524983366261936130': 'rumour', '544387669766451200': 'rumour', '524945676443340800': 'rumour', '580350403734310912': 'non-rumour', '553531343834202112': 'rumour', '500277477986828288': 'rumour', '552807101442322432': 'non-rumour', '499705915227271168': 'rumour', '544299439087550464': 'rumour', '553589860880375808': 'non-rumour', '524983146266505216': 'rumour', '552805175874686977': 'non-rumour', '553464709899616257': 'non-rumour', '553144712920834048': 'rumour', '500364921607974913': 'non-rumour', '525052223030845440': 'rumour', '544499449645768706': 'non-rumour', '500396427378298880': 'non-rumour', '544309283048144897': 'non-rumour', '500378522788315137': 'rumour', '500366417674268673': 'non-rumour', '553214358948683776': 'non-rumour', '552806937470201856': 'non-rumour', '553231516290269184': 'non-rumour', '499581876668227584': 'non-rumour', '544518337498058753': 'non-rumour', '552805069956329472': 'non-rumour', '524937330923417600': 'rumour', '525038296921960449': 'non-rumour', '553466230989144064': 'rumour', '544419162001383424': 'non-rumour', '524966596897685504': 'non-rumour', '580328708675887104': 'non-rumour', '524969878823137280': 'rumour', '553548567420628992': 'rumour', '524935769614331904': 'rumour', '499592320900431872': 'non-rumour', '544274934835707905': 'rumour', '552790598823186432': 'non-rumour', '553590835850514433': 'rumour', '552811438994378754': 'non-rumour', '544272134764122112': 'non-rumour', '552791332935434242': 'non-rumour', '499703127193305088': 'non-rumour', '553588494661337089': 'non-rumour', '499660656769921025': 'non-rumour', '553118384540643328': 'non-rumour', '500288349924782080': 'rumour', '544296985910861824': 'rumour', '544487535418687488': 'rumour', '544409400941625344': 'non-rumour', '553203041705664513': 'non-rumour', '499658267493019651': 'non-rumour', '499397804620386304': 'non-rumour', '524935246647926784': 'rumour', '553172769819852800': 'non-rumour', '498300128088694786': 'non-rumour', '544516940367413248': 'rumour', '525034457086959616': 'non-rumour', '524935085863481344': 'rumour', '553519997956669440': 'non-rumour', '580336069192843264': 'non-rumour', '553553331671408641': 'non-rumour', '544380899639689216': 'non-rumour', '524944788525973505': 'non-rumour', '581247335998431232': 'non-rumour', '544470398881259520': 'non-rumour', '580325043705667586': 'rumour', '500316380063748096': 'rumour', '525067386849091584': 'non-rumour', '580346858846822400': 'non-rumour', '553589606307090432': 'rumour', '553581669710831617': 'non-rumour', '525006356731166720': 'rumour', '553124268196642816': 'rumour', '552839416620658688': 'rumour', '553552825431883776': 'rumour', '553229948153262081': 'non-rumour', '544518415952539649': 'non-rumour', '544384394337611776': 'rumour', '524973811092193280': 'non-rumour', '544323450270392322': 'non-rumour', '544426670095368192': 'non-rumour', '553171877464920064': 'non-rumour', '552843285468020736': 'non-rumour', '553541988490825728': 'rumour', '553561169923829760': 'non-rumour', '525035552643751936': 'rumour', '553581227165642752': 'non-rumour', '552816302780579840': 'non-rumour', '580350000074457088': 'rumour', '498584409055174656': 'non-rumour', '524961070465945600': 'non-rumour'}\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}