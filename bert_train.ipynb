{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyN3H5sDypgfyXWDjUdn8BIA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JasonLaux/nlp/blob/main/bert_train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1d6Ig15COmUT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55cad9b9-3163-4c26-9dbe-29b98abf3fb0"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import BertModel, BertTokenizer\n",
        "import torch.optim as optim\n",
        "import time\n",
        "from torch.utils.data import IterableDataset, DataLoader\n",
        "import json\n",
        "from collections import OrderedDict, deque\n",
        "import datetime\n",
        "import gc "
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dcRNBLUUUptt",
        "outputId": "1fe7c813-bed2-4b66-c9b0-4bbaa105a81e"
      },
      "source": [
        "!pip install torch torchvision transformers"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.8.1+cu101)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (0.9.1+cu101)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.5.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.7.4.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch) (1.19.5)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision) (7.1.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.10.1)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQGyMorkTWSu"
      },
      "source": [
        "DATA_TRAIN_PATH = './gdrive/MyDrive/nlp/train.data.jsonl'\n",
        "LABEL_TRAIN_PATH = './gdrive/MyDrive/nlp/train.label.json'\n",
        "DATA_DEV_PATH = './gdrive/MyDrive/nlp/dev.data.jsonl'\n",
        "LABEL_DEV_PATH = './gdrive/MyDrive/nlp/dev.label.json'"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "anNWiP-bQAe5"
      },
      "source": [
        "class DAG(object):\n",
        "    \"\"\" Directed acyclic graph implementation. \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\" Construct a new DAG with no nodes or edges. \"\"\"\n",
        "        self.node_depth = []\n",
        "        self.graph = OrderedDict()\n",
        "\n",
        "    def add_node(self, node: tuple):\n",
        "        \"\"\" Add a node if it does not exist yet, or error out. \"\"\"\n",
        "        if node in self.graph:\n",
        "            raise KeyError('node %s already exists' % node.index)\n",
        "        self.graph[node] = set()\n",
        "\n",
        "    def add_edge(self, start_node: tuple, end_node: tuple):\n",
        "        \"\"\" Add an edge (dependency) between the specified nodes. \"\"\"\n",
        "\n",
        "        if start_node not in self.graph or end_node not in self.graph:\n",
        "            raise KeyError(\"Node is not existed in the graph.\")\n",
        "\n",
        "        self.graph[start_node].add(end_node)\n",
        "\n",
        "    # Sort children node by time in ascending order\n",
        "    def sort_children(self):\n",
        "        for key in self.graph:\n",
        "            self.graph[key] = sorted(self.graph[key], key=lambda item: item[1])\n",
        "\n",
        "    def get_graph_dict(self):\n",
        "        return self.graph\n",
        "\n",
        "    def predecessors(self, node: tuple):\n",
        "        \"\"\" Returns a list of all predecessors of the given node \"\"\"\n",
        "\n",
        "        return [key for key in self.graph if node in self.graph[key]]\n",
        "\n",
        "    def downstream(self, node: tuple):\n",
        "        \"\"\" Returns a list of all nodes this node has edges towards. \"\"\"\n",
        "        if node.index not in self.graph:\n",
        "            raise KeyError('node %s is not in graph' % node.index)\n",
        "        return list(self.graph[node.index])\n",
        "\n",
        "    def all_downstreams(self, node, graph=None):\n",
        "        \"\"\"Returns a list of all nodes ultimately downstream\n",
        "        of the given node in the dependency graph, in\n",
        "        topological order.\"\"\"\n",
        "        if graph is None:\n",
        "            graph = self.graph\n",
        "        nodes = [node]\n",
        "        nodes_seen = set()\n",
        "        i = 0\n",
        "        while i < len(nodes):\n",
        "            downstreams = self.downstream(nodes[i], graph)\n",
        "            for downstream_node in downstreams:\n",
        "                if downstream_node not in nodes_seen:\n",
        "                    nodes_seen.add(downstream_node)\n",
        "                    nodes.append(downstream_node)\n",
        "            i += 1\n",
        "        return list(\n",
        "            filter(\n",
        "                lambda node: node in nodes_seen,\n",
        "                self.topological_sort(graph=graph)\n",
        "            )\n",
        "        )\n",
        "\n",
        "    def all_leaves(self, graph=None):\n",
        "        \"\"\" Return a list of all leaves (nodes with no downstreams) \"\"\"\n",
        "        if graph is None:\n",
        "            graph = self.graph\n",
        "        return [key for key in graph if not graph[key]]\n",
        "\n",
        "    def topological_sort(self):  # 返回从根到叶子的排序列表\n",
        "        \"\"\" Returns a topological ordering of the DAG.\n",
        "        Raises an error if this is not possible (graph is not valid).\n",
        "        \"\"\"\n",
        "\n",
        "        in_degree = {}\n",
        "        for u in self.graph:\n",
        "            in_degree[u] = 0\n",
        "\n",
        "        for u in self.graph:\n",
        "            for v in self.graph[u]:\n",
        "                in_degree[v] += 1\n",
        "\n",
        "        queue = deque()\n",
        "        for u in in_degree:\n",
        "            if in_degree[u] == 0:\n",
        "                queue.appendleft(u)\n",
        "\n",
        "        l = []\n",
        "        while queue:\n",
        "            u = queue.pop()\n",
        "            l.append(u)\n",
        "            for v in self.graph[u]:\n",
        "                in_degree[v] -= 1\n",
        "                if in_degree[v] == 0:\n",
        "                    queue.appendleft(v)\n",
        "\n",
        "        if len(l) == len(self.graph):\n",
        "            return l\n",
        "        else:\n",
        "            raise ValueError('graph is not acyclic')\n",
        "\n",
        "    def dag_depth(self, node, graph=None, depth=0):\n",
        "        if self.graph == {}:\n",
        "            return 0\n",
        "        nodes = self.downstream(node)\n",
        "        if len(nodes) == 0:\n",
        "            self.node_depth.append(depth)\n",
        "        else:\n",
        "            for node in nodes:\n",
        "                self.dag_depth(node, self.graph, depth + 1)\n",
        "        return max(self.node_depth)\n",
        "\n",
        "    def size(self):\n",
        "        return len(self.graph)\n",
        "\n",
        "\n",
        "def create_graph(items):\n",
        "    graph = DAG()\n",
        "    root_time = \"\"\n",
        "    idStr_idx = {}\n",
        "\n",
        "    for idx, item in enumerate(items):\n",
        "        if idx == 0:\n",
        "            root_time = item[\"created_at\"]\n",
        "        commit_time = item[\"created_at\"]\n",
        "        idStr_idx.update({item[\"id_str\"]: idx})  # Assuming every tweet is unique\n",
        "        graph.add_node((idx, calc_time_diff(root_time, commit_time)))\n",
        "\n",
        "    keys = list(graph.graph.copy())\n",
        "\n",
        "    for idx, item in enumerate(items):\n",
        "        parent_idx = idStr_idx.get(str(item[\"in_reply_to_status_id_str\"]))\n",
        "        if parent_idx is not None:\n",
        "            graph.add_edge(keys[parent_idx], keys[idx])\n",
        "\n",
        "    graph.sort_children()\n",
        "\n",
        "    return graph\n",
        "\n",
        "\n",
        "def calc_time_diff(start_time: str, end_time: str):\n",
        "    start_time_formatted = datetime.datetime.strptime(start_time, \"%a %b %d %H:%M:%S %z %Y\")\n",
        "    end_time_formatted = datetime.datetime.strptime(end_time, \"%a %b %d %H:%M:%S %z %Y\")\n",
        "    return (end_time_formatted - start_time_formatted).total_seconds()"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4QulgoC-QOZO"
      },
      "source": [
        "class TweetDataset(IterableDataset):\n",
        "\n",
        "    def __init__(self, fn_data, fn_label, maxlen):\n",
        "        # Store the contents of the file in a pandas dataframe\n",
        "        self.data_reader = open(fn_data, encoding=\"utf-8\")\n",
        "        self.label_dict = json.load(open(fn_label, encoding=\"utf-8\"))\n",
        "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "        # Initialize the BERT tokenizer\n",
        "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "        self.maxlen = maxlen  # the max length of the sentence in the corpus\n",
        "\n",
        "    def __iter__(self):\n",
        "\n",
        "        for line in self.data_reader:\n",
        "            line = line.strip()\n",
        "            if line.endswith(']'):\n",
        "                items = json.loads(line)\n",
        "                id_str = items[0][\"id_str\"]  # Tweet index\n",
        "                idx_list = [pair[0] for pair in create_graph(items).topological_sort()]\n",
        "                tokens_list = []\n",
        "                for idx in idx_list:\n",
        "                    username = items[idx][\"user\"][\"name\"]\n",
        "                    text = items[idx][\"text\"]\n",
        "                    current_sentence = username + \":\" + text\n",
        "                    tokens_list.append(self.tokenizer.tokenize(current_sentence))\n",
        "\n",
        "                tokens_concat = ['[CLS]'] + [token for item in tokens_list for token in item] + ['[SEP]']\n",
        "                if len(tokens_concat) < self.maxlen:\n",
        "                    padded_tokens = tokens_concat + ['[PAD]' for _ in range(self.maxlen - len(tokens_concat))]\n",
        "                else:\n",
        "                    padded_tokens = tokens_concat[:self.maxlen - 1] + ['[SEP]']\n",
        "\n",
        "                tokens_ids = self.tokenizer.convert_tokens_to_ids(padded_tokens)\n",
        "                # attn_mask = [1 if token != '[PAD]' else 0 for token in padded_tokens]\n",
        "                tokens_ids_tensor = torch.tensor(tokens_ids)\n",
        "                attn_mask = (tokens_ids_tensor != 0).long()\n",
        "                label = self.label_dict[id_str]  # Tweet label\n",
        "                if label == \"non-rumour\":\n",
        "                  label_idx = torch.tensor(1)\n",
        "                else:\n",
        "                  label_idx = torch.tensor(0)\n",
        "                yield tokens_ids_tensor, attn_mask, label_idx\n",
        "            else:\n",
        "                raise KeyError(\"Lines are not in format!\")"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BbxrXgpJQPmR"
      },
      "source": [
        "class RumourClassifier(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(RumourClassifier, self).__init__()\n",
        "        # Instantiating BERT model object\n",
        "        self.bert_layer = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "        # Classification layer\n",
        "        # input dimension is 768 because [CLS] embedding has a dimension of 768\n",
        "        # output dimension is 1 because we're working with a binary classification problem\n",
        "        self.cls_layer = nn.Linear(768, 1)\n",
        "\n",
        "    def forward(self, tokens_ids, attn_masks):\n",
        "        '''\n",
        "        Inputs:\n",
        "            -seq : Tensor of shape [B, T] containing token ids of sequences\n",
        "            -attn_masks : Tensor of shape [B, T] containing attention masks to be used to avoid contibution of PAD tokens\n",
        "        '''\n",
        "\n",
        "        # Feeding the input to BERT model to obtain contextualized representations\n",
        "        outputs = self.bert_layer(input_ids=tokens_ids, attention_mask=attn_masks)\n",
        "        cont_reps = outputs.last_hidden_state\n",
        "\n",
        "        # Obtaining the representation of [CLS] head (the first token)\n",
        "        cls_rep = cont_reps[:, 0]\n",
        "\n",
        "        # Feeding cls_rep to the classifier layer\n",
        "        logits = self.cls_layer(cls_rep)\n",
        "\n",
        "        return logits\n",
        "\n",
        "\n",
        "def get_accuracy_from_logits(logits, labels):\n",
        "    probs = torch.sigmoid(logits.unsqueeze(-1))\n",
        "    soft_probs = (probs > 0.5).long()\n",
        "    acc = (soft_probs.squeeze() == labels).float().mean()\n",
        "    return acc\n",
        "\n",
        "\n",
        "def evaluate(net, criterion, dataloader, gpu):\n",
        "    net.eval()\n",
        "\n",
        "    mean_acc, mean_loss = 0, 0\n",
        "    count = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for seq, attn_masks, labels in dataloader:\n",
        "            seq, attn_masks, labels = seq.cuda(gpu), attn_masks.cuda(gpu), labels.cuda(gpu)\n",
        "            logits = net(seq, attn_masks)\n",
        "            mean_loss += criterion(logits.squeeze(-1), labels.float()).item()\n",
        "            mean_acc += get_accuracy_from_logits(logits, labels)\n",
        "            count += 1\n",
        "    \n",
        "    if count == 0:\n",
        "      raise KeyError(\"Dataloader is loaded incorrectly!\")\n",
        "    else:\n",
        "      return mean_acc / count, mean_loss / count\n",
        "\n",
        "\n",
        "def train(net, criterion, opti, train_loader, dev_loader, max_eps, gpu):\n",
        "    best_acc = 0\n",
        "    st = time.time()\n",
        "    for ep in range(max_eps):\n",
        "      \n",
        "\n",
        "        for it, (seq, attn_masks, labels) in enumerate(train_loader):\n",
        "            # Clear gradients\n",
        "            opti.zero_grad()\n",
        "            # Converting these to cuda tensors\n",
        "            seq = seq.cuda(gpu)\n",
        "            attn_masks = attn_masks.cuda(gpu)\n",
        "            labels = labels.cuda(gpu)\n",
        "\n",
        "            # Obtaining the logits from the model\n",
        "            logits = net(seq, attn_masks)    #########################\n",
        "\n",
        "            # Computing loss\n",
        "            loss = criterion(logits.squeeze(-1), labels.float())\n",
        "\n",
        "            # Backpropagating the gradients\n",
        "            loss.backward()\n",
        "\n",
        "            # Optimization step\n",
        "            opti.step()\n",
        "\n",
        "            if it % 100 == 0:\n",
        "                acc = get_accuracy_from_logits(logits, labels)\n",
        "                print(\"Iteration {} of epoch {} complete. Loss: {}; Accuracy: {}; Time taken (s): {}\".format(it, ep,\n",
        "                                                                                                             loss.item(),\n",
        "                                                                                                             acc, (\n",
        "                                                                                                                     time.time() - st)))\n",
        "                st = time.time()\n",
        "\n",
        "        dev_acc, dev_loss = evaluate(net, criterion, dev_loader, gpu)\n",
        "        print(\"Epoch {} complete! Development Accuracy: {}; Development Loss: {}\".format(ep, dev_acc, dev_loss))\n",
        "        if dev_acc > best_acc:\n",
        "            print(\"Best development accuracy improved from {} to {}, saving model...\".format(best_acc, dev_acc))\n",
        "            best_acc = dev_acc\n",
        "            torch.save(net.state_dict(), 'sstcls_{}.dat'.format(ep))"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1s2Wq7RsjpZ2"
      },
      "source": [
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BjKJ36JoQVE2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 623
        },
        "outputId": "7f64d4e4-ea8a-47b1-91e2-37013065a0ca"
      },
      "source": [
        "gpu = 0\n",
        "print(\"Creating the classifier, initialised with pretrained BERT-BASE parameters...\")\n",
        "net = RumourClassifier()\n",
        "net.cuda(gpu)  # Enable gpu support for the model\n",
        "print(\"Done creating the classifier.\")\n",
        "# Define loss function based on binary cross-entropy.\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "opti = optim.Adam(net.parameters(), lr=2e-5)\n",
        "num_epoch = 10\n",
        "\n",
        "train_dataset = TweetDataset(DATA_TRAIN_PATH, LABEL_TRAIN_PATH, maxlen=256)\n",
        "dev_dataset = TweetDataset(DATA_DEV_PATH, LABEL_DEV_PATH, maxlen=256)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=10)\n",
        "dev_dataloader = DataLoader(dev_dataset, batch_size=10)\n",
        "\n",
        "train(net, criterion, opti, train_dataloader, dev_dataloader, num_epoch, gpu)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Creating the classifier, initialised with pretrained BERT-BASE parameters...\n",
            "Done creating the classifier.\n",
            "Iteration 0 of epoch 0 complete. Loss: 0.7377568483352661; Accuracy: 0.20000000298023224; Time taken (s): 0.6591119766235352\n",
            "Iteration 100 of epoch 0 complete. Loss: 0.7595022320747375; Accuracy: 0.30000001192092896; Time taken (s): 60.33205556869507\n",
            "Iteration 200 of epoch 0 complete. Loss: 0.26297426223754883; Accuracy: 0.9000000357627869; Time taken (s): 62.2986478805542\n",
            "Iteration 300 of epoch 0 complete. Loss: 0.6665245890617371; Accuracy: 0.800000011920929; Time taken (s): 62.651978731155396\n",
            "Iteration 400 of epoch 0 complete. Loss: 0.6295601725578308; Accuracy: 0.800000011920929; Time taken (s): 62.283337116241455\n",
            "Epoch 0 complete! Development Accuracy: 0.877586305141449; Development Loss: 0.29342944740221416\n",
            "Best development accuracy improved from 0 to 0.877586305141449, saving model...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-48-c48f8e415517>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mdev_dataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdev_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopti\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgpu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-46-d1037d8ed22e>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(net, criterion, opti, train_loader, dev_loader, max_eps, gpu)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mdev_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgpu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Epoch {} complete! Development Accuracy: {}; Development Loss: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdev_acc\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbest_acc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-46-d1037d8ed22e>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(net, criterion, dataloader, gpu)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Dataloader is loaded incorrectly!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mmean_acc\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean_loss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'Dataloader is loaded incorrectly!'"
          ]
        }
      ]
    }
  ]
}