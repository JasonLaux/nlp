{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMA8fs4gbK6no9AtYZuaz0O",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JasonLaux/nlp/blob/main/bert_train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1d6Ig15COmUT",
        "outputId": "488d9d81-4a47-426f-d0d9-fb90b2a8a09e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import BertModel, BertTokenizer\n",
        "import torch.optim as optim\n",
        "import time\n",
        "from torch.utils.data import IterableDataset, DataLoader\n",
        "import json\n",
        "from collections import OrderedDict, deque\n",
        "import datetime"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dcRNBLUUUptt"
      },
      "source": [
        "!pip install torch torchvision transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQGyMorkTWSu"
      },
      "source": [
        "DATA_TRAIN_PATH = './gdrive/MyDrive/nlp/train.data.jsonl'\n",
        "LABEL_TRAIN_PATH = './gdrive/MyDrive/nlp/train.label.json'\n",
        "DATA_DEV_PATH = './gdrive/MyDrive/nlp/dev.data.jsonl'\n",
        "LABEL_DEV_PATH = './gdrive/MyDrive/nlp/dev.label.json'"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "anNWiP-bQAe5"
      },
      "source": [
        "class DAG(object):\n",
        "    \"\"\" Directed acyclic graph implementation. \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\" Construct a new DAG with no nodes or edges. \"\"\"\n",
        "        self.node_depth = []\n",
        "        self.graph = OrderedDict()\n",
        "\n",
        "    def add_node(self, node: tuple):\n",
        "        \"\"\" Add a node if it does not exist yet, or error out. \"\"\"\n",
        "        if node in self.graph:\n",
        "            raise KeyError('node %s already exists' % node.index)\n",
        "        self.graph[node] = set()\n",
        "\n",
        "    def add_edge(self, start_node: tuple, end_node: tuple):\n",
        "        \"\"\" Add an edge (dependency) between the specified nodes. \"\"\"\n",
        "\n",
        "        if start_node not in self.graph or end_node not in self.graph:\n",
        "            raise KeyError(\"Node is not existed in the graph.\")\n",
        "\n",
        "        self.graph[start_node].add(end_node)\n",
        "\n",
        "    # Sort children node by time in ascending order\n",
        "    def sort_children(self):\n",
        "        for key in self.graph:\n",
        "            self.graph[key] = sorted(self.graph[key], key=lambda item: item[1])\n",
        "\n",
        "    def get_graph_dict(self):\n",
        "        return self.graph\n",
        "\n",
        "    def predecessors(self, node: tuple):\n",
        "        \"\"\" Returns a list of all predecessors of the given node \"\"\"\n",
        "\n",
        "        return [key for key in self.graph if node in self.graph[key]]\n",
        "\n",
        "    def downstream(self, node: tuple):\n",
        "        \"\"\" Returns a list of all nodes this node has edges towards. \"\"\"\n",
        "        if node.index not in self.graph:\n",
        "            raise KeyError('node %s is not in graph' % node.index)\n",
        "        return list(self.graph[node.index])\n",
        "\n",
        "    def all_downstreams(self, node, graph=None):\n",
        "        \"\"\"Returns a list of all nodes ultimately downstream\n",
        "        of the given node in the dependency graph, in\n",
        "        topological order.\"\"\"\n",
        "        if graph is None:\n",
        "            graph = self.graph\n",
        "        nodes = [node]\n",
        "        nodes_seen = set()\n",
        "        i = 0\n",
        "        while i < len(nodes):\n",
        "            downstreams = self.downstream(nodes[i], graph)\n",
        "            for downstream_node in downstreams:\n",
        "                if downstream_node not in nodes_seen:\n",
        "                    nodes_seen.add(downstream_node)\n",
        "                    nodes.append(downstream_node)\n",
        "            i += 1\n",
        "        return list(\n",
        "            filter(\n",
        "                lambda node: node in nodes_seen,\n",
        "                self.topological_sort(graph=graph)\n",
        "            )\n",
        "        )\n",
        "\n",
        "    def all_leaves(self, graph=None):\n",
        "        \"\"\" Return a list of all leaves (nodes with no downstreams) \"\"\"\n",
        "        if graph is None:\n",
        "            graph = self.graph\n",
        "        return [key for key in graph if not graph[key]]\n",
        "\n",
        "    def topological_sort(self):  # 返回从根到叶子的排序列表\n",
        "        \"\"\" Returns a topological ordering of the DAG.\n",
        "        Raises an error if this is not possible (graph is not valid).\n",
        "        \"\"\"\n",
        "\n",
        "        in_degree = {}\n",
        "        for u in self.graph:\n",
        "            in_degree[u] = 0\n",
        "\n",
        "        for u in self.graph:\n",
        "            for v in self.graph[u]:\n",
        "                in_degree[v] += 1\n",
        "\n",
        "        queue = deque()\n",
        "        for u in in_degree:\n",
        "            if in_degree[u] == 0:\n",
        "                queue.appendleft(u)\n",
        "\n",
        "        l = []\n",
        "        while queue:\n",
        "            u = queue.pop()\n",
        "            l.append(u)\n",
        "            for v in self.graph[u]:\n",
        "                in_degree[v] -= 1\n",
        "                if in_degree[v] == 0:\n",
        "                    queue.appendleft(v)\n",
        "\n",
        "        if len(l) == len(self.graph):\n",
        "            return l\n",
        "        else:\n",
        "            raise ValueError('graph is not acyclic')\n",
        "\n",
        "    def dag_depth(self, node, graph=None, depth=0):\n",
        "        if self.graph == {}:\n",
        "            return 0\n",
        "        nodes = self.downstream(node)\n",
        "        if len(nodes) == 0:\n",
        "            self.node_depth.append(depth)\n",
        "        else:\n",
        "            for node in nodes:\n",
        "                self.dag_depth(node, self.graph, depth + 1)\n",
        "        return max(self.node_depth)\n",
        "\n",
        "    def size(self):\n",
        "        return len(self.graph)\n",
        "\n",
        "\n",
        "def create_graph(items):\n",
        "    graph = DAG()\n",
        "    root_time = \"\"\n",
        "    idStr_idx = {}\n",
        "\n",
        "    for idx, item in enumerate(items):\n",
        "        if idx == 0:\n",
        "            root_time = item[\"created_at\"]\n",
        "        commit_time = item[\"created_at\"]\n",
        "        idStr_idx.update({item[\"id_str\"]: idx})  # Assuming every tweet is unique\n",
        "        graph.add_node((idx, calc_time_diff(root_time, commit_time)))\n",
        "\n",
        "    keys = list(graph.graph.copy())\n",
        "\n",
        "    for idx, item in enumerate(items):\n",
        "        parent_idx = idStr_idx.get(str(item[\"in_reply_to_status_id_str\"]))\n",
        "        if parent_idx is not None:\n",
        "            graph.add_edge(keys[parent_idx], keys[idx])\n",
        "\n",
        "    graph.sort_children()\n",
        "\n",
        "    return graph\n",
        "\n",
        "\n",
        "def calc_time_diff(start_time: str, end_time: str):\n",
        "    start_time_formatted = datetime.datetime.strptime(start_time, \"%a %b %d %H:%M:%S %z %Y\")\n",
        "    end_time_formatted = datetime.datetime.strptime(end_time, \"%a %b %d %H:%M:%S %z %Y\")\n",
        "    return (end_time_formatted - start_time_formatted).total_seconds()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4QulgoC-QOZO"
      },
      "source": [
        "class TweetDataset(IterableDataset):\n",
        "\n",
        "    def __init__(self, fn_data, fn_label, maxlen):\n",
        "        # Store the contents of the file in a pandas dataframe\n",
        "        self.data_reader = open(fn_data, encoding=\"utf-8\")\n",
        "        self.label_dict = json.load(open(fn_label, encoding=\"utf-8\"))\n",
        "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "        # Initialize the BERT tokenizer\n",
        "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "        self.maxlen = maxlen  # the max length of the sentence in the corpus\n",
        "\n",
        "    def __iter__(self):\n",
        "\n",
        "        for line in self.data_reader:\n",
        "            line = line.strip()\n",
        "            if line.endswith(']'):\n",
        "                items = json.loads(line)\n",
        "                id_str = items[0][\"id_str\"]  # Tweet index\n",
        "                idx_list = [pair[0] for pair in create_graph(items).topological_sort()]\n",
        "                tokens_list = []\n",
        "                for idx in idx_list:\n",
        "                    username = items[idx][\"user\"][\"name\"]\n",
        "                    text = items[idx][\"text\"]\n",
        "                    current_sentence = username + \":\" + text\n",
        "                    tokens_list.append(self.tokenizer.tokenize(current_sentence))\n",
        "\n",
        "                tokens_concat = ['[CLS]'] + [token for item in tokens_list for token in item] + ['[SEP]']\n",
        "                if len(tokens_concat) < self.maxlen:\n",
        "                    padded_tokens = tokens_concat + ['[PAD]' for _ in range(self.maxlen - len(tokens_concat))]\n",
        "                else:\n",
        "                    padded_tokens = tokens_concat[:self.maxlen - 1] + ['[SEP]']\n",
        "\n",
        "                tokens_ids = self.tokenizer.convert_tokens_to_ids(padded_tokens)\n",
        "                # attn_mask = [1 if token != '[PAD]' else 0 for token in padded_tokens]\n",
        "                tokens_ids_tensor = torch.tensor(tokens_ids)\n",
        "                attn_mask = (tokens_ids_tensor != 0).long()\n",
        "                label = self.label_dict[id_str]  # Tweet label\n",
        "                if label == \"non-rumour\":\n",
        "                  label_idx = torch.tensor(1)\n",
        "                else:\n",
        "                  label_idx = torch.tensor(0)\n",
        "                yield tokens_ids_tensor, attn_mask, label_idx\n",
        "            else:\n",
        "                raise KeyError(\"Lines are not in format!\")"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BbxrXgpJQPmR"
      },
      "source": [
        "class RumourClassifier(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(RumourClassifier, self).__init__()\n",
        "        # Instantiating BERT model object\n",
        "        self.bert_layer = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "        # Classification layer\n",
        "        # input dimension is 768 because [CLS] embedding has a dimension of 768\n",
        "        # output dimension is 1 because we're working with a binary classification problem\n",
        "        self.cls_layer = nn.Linear(768, 1)\n",
        "\n",
        "    def forward(self, tokens_ids, attn_masks):\n",
        "        '''\n",
        "        Inputs:\n",
        "            -seq : Tensor of shape [B, T] containing token ids of sequences\n",
        "            -attn_masks : Tensor of shape [B, T] containing attention masks to be used to avoid contibution of PAD tokens\n",
        "        '''\n",
        "\n",
        "        # Feeding the input to BERT model to obtain contextualized representations\n",
        "        outputs = self.bert_layer(input_ids=tokens_ids, attention_mask=attn_masks)\n",
        "        cont_reps = outputs.last_hidden_state\n",
        "\n",
        "        # Obtaining the representation of [CLS] head (the first token)\n",
        "        cls_rep = cont_reps[:, 0]\n",
        "\n",
        "        # Feeding cls_rep to the classifier layer\n",
        "        logits = self.cls_layer(cls_rep)\n",
        "\n",
        "        return logits\n",
        "\n",
        "\n",
        "def get_accuracy_from_logits(logits, labels):\n",
        "    probs = torch.sigmoid(logits.unsqueeze(-1))\n",
        "    soft_probs = (probs > 0.5).long()\n",
        "    acc = (soft_probs.squeeze() == labels).float().mean()\n",
        "    return acc\n",
        "\n",
        "\n",
        "def evaluate(net, criterion, dataloader, gpu):\n",
        "    net.eval()\n",
        "\n",
        "    mean_acc, mean_loss = 0, 0\n",
        "    count = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for seq, attn_masks, labels in dataloader:\n",
        "            seq, attn_masks, labels = seq.cuda(gpu), attn_masks.cuda(gpu), labels.cuda(gpu)\n",
        "            logits = net(seq, attn_masks)\n",
        "            mean_loss += criterion(logits.squeeze(-1), labels.float()).item()\n",
        "            mean_acc += get_accuracy_from_logits(logits, labels)\n",
        "            count += 1\n",
        "\n",
        "    return mean_acc / count, mean_loss / count\n",
        "\n",
        "\n",
        "def train(net, criterion, opti, train_loader, dev_loader, max_eps, gpu):\n",
        "    best_acc = 0\n",
        "    st = time.time()\n",
        "    for ep in range(max_eps):\n",
        "\n",
        "        for it, (seq, attn_masks, labels) in enumerate(train_loader):\n",
        "            # Clear gradients\n",
        "            opti.zero_grad()\n",
        "            # Converting these to cuda tensors\n",
        "            seq = seq.cuda(gpu)\n",
        "            attn_masks = attn_masks.cuda(gpu)\n",
        "            labels = labels.cuda(gpu)\n",
        "\n",
        "            # Obtaining the logits from the model\n",
        "            logits = net(seq, attn_masks)    #########################\n",
        "\n",
        "            # Computing loss\n",
        "            loss = criterion(logits.squeeze(-1), labels.float())\n",
        "\n",
        "            # Backpropagating the gradients\n",
        "            loss.backward()\n",
        "\n",
        "            # Optimization step\n",
        "            opti.step()\n",
        "\n",
        "            if it % 100 == 0:\n",
        "                acc = get_accuracy_from_logits(logits, labels)\n",
        "                print(\"Iteration {} of epoch {} complete. Loss: {}; Accuracy: {}; Time taken (s): {}\".format(it, ep,\n",
        "                                                                                                             loss.item(),\n",
        "                                                                                                             acc, (\n",
        "                                                                                                                     time.time() - st)))\n",
        "                st = time.time()\n",
        "\n",
        "        dev_acc, dev_loss = evaluate(net, criterion, dev_loader, gpu)\n",
        "        print(\"Epoch {} complete! Development Accuracy: {}; Development Loss: {}\".format(ep, dev_acc, dev_loss))\n",
        "        if dev_acc > best_acc:\n",
        "            print(\"Best development accuracy improved from {} to {}, saving model...\".format(best_acc, dev_acc))\n",
        "            best_acc = dev_acc\n",
        "            torch.save(net.state_dict(), 'sstcls_{}.dat'.format(ep))"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BjKJ36JoQVE2",
        "outputId": "812bf6ac-b224-45f0-f30a-98542ed5ee67",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        }
      },
      "source": [
        "gpu = 0\n",
        "print(\"Creating the classifier, initialised with pretrained BERT-BASE parameters...\")\n",
        "net = RumourClassifier()\n",
        "net.cuda(gpu)  # Enable gpu support for the model\n",
        "print(\"Done creating the classifier.\")\n",
        "# Define loss function based on binary cross-entropy.\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "opti = optim.Adam(net.parameters(), lr=2e-5)\n",
        "num_epoch = 30\n",
        "\n",
        "train_dataset = TweetDataset(DATA_TRAIN_PATH, LABEL_TRAIN_PATH, maxlen=512)\n",
        "dev_dataset = TweetDataset(DATA_DEV_PATH, LABEL_DEV_PATH, maxlen=512)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=10)\n",
        "dev_dataloader = DataLoader(dev_dataset, batch_size=10)\n",
        "\n",
        "train(net, criterion, opti, train_dataloader, dev_dataloader, num_epoch, gpu)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Creating the classifier, initialised with pretrained BERT-BASE parameters...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-7cbb56402112>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Creating the classifier, initialised with pretrained BERT-BASE parameters...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRumourClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgpu\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Enable gpu support for the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Done creating the classifier.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Define loss function based on binary cross-entropy.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mcuda\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0mModule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \"\"\"\n\u001b[0;32m--> 491\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mxpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    385\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 387\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    385\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 387\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    385\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 387\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    407\u001b[0m                 \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 409\u001b[0;31m                     \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    410\u001b[0m                 \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0mModule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \"\"\"\n\u001b[0;32m--> 491\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mxpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: invalid device ordinal"
          ]
        }
      ]
    }
  ]
}