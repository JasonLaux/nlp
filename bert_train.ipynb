{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMTorLzuC3XhE6JG16azhpM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JasonLaux/nlp/blob/main/bert_train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1d6Ig15COmUT"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import BertModel, BertTokenizer\n",
        "import torch.optim as optim\n",
        "import time\n",
        "from torch.utils.data import IterableDataset, DataLoader\n",
        "import json\n",
        "from graph_sort import create_graph\n",
        "from collections import OrderedDict, deque\n",
        "import datetime\n",
        "DATA_TRAIN_PATH = './data/train.data.jsonl'\n",
        "LABEL_TRAIN_PATH = './data/train.label.json'\n",
        "DATA_DEV_PATH = './data/dev.data.jsonl'\n",
        "LABEL_DEV_PATH = './data/dev.label.json'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "anNWiP-bQAe5"
      },
      "source": [
        "class DAG(object):\n",
        "    \"\"\" Directed acyclic graph implementation. \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\" Construct a new DAG with no nodes or edges. \"\"\"\n",
        "        self.node_depth = []\n",
        "        self.graph = OrderedDict()\n",
        "\n",
        "    def add_node(self, node: tuple):\n",
        "        \"\"\" Add a node if it does not exist yet, or error out. \"\"\"\n",
        "        if node in self.graph:\n",
        "            raise KeyError('node %s already exists' % node.index)\n",
        "        self.graph[node] = set()\n",
        "\n",
        "    def add_edge(self, start_node: tuple, end_node: tuple):\n",
        "        \"\"\" Add an edge (dependency) between the specified nodes. \"\"\"\n",
        "\n",
        "        if start_node not in self.graph or end_node not in self.graph:\n",
        "            raise KeyError(\"Node is not existed in the graph.\")\n",
        "\n",
        "        self.graph[start_node].add(end_node)\n",
        "\n",
        "    # Sort children node by time in ascending order\n",
        "    def sort_children(self):\n",
        "        for key in self.graph:\n",
        "            self.graph[key] = sorted(self.graph[key], key=lambda item: item[1])\n",
        "\n",
        "    def get_graph_dict(self):\n",
        "        return self.graph\n",
        "\n",
        "    def predecessors(self, node: tuple):\n",
        "        \"\"\" Returns a list of all predecessors of the given node \"\"\"\n",
        "\n",
        "        return [key for key in self.graph if node in self.graph[key]]\n",
        "\n",
        "    def downstream(self, node: tuple):\n",
        "        \"\"\" Returns a list of all nodes this node has edges towards. \"\"\"\n",
        "        if node.index not in self.graph:\n",
        "            raise KeyError('node %s is not in graph' % node.index)\n",
        "        return list(self.graph[node.index])\n",
        "\n",
        "    def all_downstreams(self, node, graph=None):\n",
        "        \"\"\"Returns a list of all nodes ultimately downstream\n",
        "        of the given node in the dependency graph, in\n",
        "        topological order.\"\"\"\n",
        "        if graph is None:\n",
        "            graph = self.graph\n",
        "        nodes = [node]\n",
        "        nodes_seen = set()\n",
        "        i = 0\n",
        "        while i < len(nodes):\n",
        "            downstreams = self.downstream(nodes[i], graph)\n",
        "            for downstream_node in downstreams:\n",
        "                if downstream_node not in nodes_seen:\n",
        "                    nodes_seen.add(downstream_node)\n",
        "                    nodes.append(downstream_node)\n",
        "            i += 1\n",
        "        return list(\n",
        "            filter(\n",
        "                lambda node: node in nodes_seen,\n",
        "                self.topological_sort(graph=graph)\n",
        "            )\n",
        "        )\n",
        "\n",
        "    def all_leaves(self, graph=None):\n",
        "        \"\"\" Return a list of all leaves (nodes with no downstreams) \"\"\"\n",
        "        if graph is None:\n",
        "            graph = self.graph\n",
        "        return [key for key in graph if not graph[key]]\n",
        "\n",
        "    def topological_sort(self):  # 返回从根到叶子的排序列表\n",
        "        \"\"\" Returns a topological ordering of the DAG.\n",
        "        Raises an error if this is not possible (graph is not valid).\n",
        "        \"\"\"\n",
        "\n",
        "        in_degree = {}\n",
        "        for u in self.graph:\n",
        "            in_degree[u] = 0\n",
        "\n",
        "        for u in self.graph:\n",
        "            for v in self.graph[u]:\n",
        "                in_degree[v] += 1\n",
        "\n",
        "        queue = deque()\n",
        "        for u in in_degree:\n",
        "            if in_degree[u] == 0:\n",
        "                queue.appendleft(u)\n",
        "\n",
        "        l = []\n",
        "        while queue:\n",
        "            u = queue.pop()\n",
        "            l.append(u)\n",
        "            for v in self.graph[u]:\n",
        "                in_degree[v] -= 1\n",
        "                if in_degree[v] == 0:\n",
        "                    queue.appendleft(v)\n",
        "\n",
        "        if len(l) == len(self.graph):\n",
        "            return l\n",
        "        else:\n",
        "            raise ValueError('graph is not acyclic')\n",
        "\n",
        "    def dag_depth(self, node, graph=None, depth=0):\n",
        "        if self.graph == {}:\n",
        "            return 0\n",
        "        nodes = self.downstream(node)\n",
        "        if len(nodes) == 0:\n",
        "            self.node_depth.append(depth)\n",
        "        else:\n",
        "            for node in nodes:\n",
        "                self.dag_depth(node, self.graph, depth + 1)\n",
        "        return max(self.node_depth)\n",
        "\n",
        "    def size(self):\n",
        "        return len(self.graph)\n",
        "\n",
        "\n",
        "def create_graph(items):\n",
        "    graph = DAG()\n",
        "    root_time = \"\"\n",
        "    idStr_idx = {}\n",
        "\n",
        "    for idx, item in enumerate(items):\n",
        "        if idx == 0:\n",
        "            root_time = item[\"created_at\"]\n",
        "        commit_time = item[\"created_at\"]\n",
        "        idStr_idx.update({item[\"id_str\"]: idx})  # Assuming every tweet is unique\n",
        "        graph.add_node((idx, calc_time_diff(root_time, commit_time)))\n",
        "\n",
        "    keys = list(graph.graph.copy())\n",
        "\n",
        "    for idx, item in enumerate(items):\n",
        "        parent_idx = idStr_idx.get(str(item[\"in_reply_to_status_id_str\"]))\n",
        "        if parent_idx is not None:\n",
        "            graph.add_edge(keys[parent_idx], keys[idx])\n",
        "\n",
        "    graph.sort_children()\n",
        "\n",
        "    return graph\n",
        "\n",
        "\n",
        "def calc_time_diff(start_time: str, end_time: str):\n",
        "    start_time_formatted = datetime.datetime.strptime(start_time, \"%a %b %d %H:%M:%S %z %Y\")\n",
        "    end_time_formatted = datetime.datetime.strptime(end_time, \"%a %b %d %H:%M:%S %z %Y\")\n",
        "    return (end_time_formatted - start_time_formatted).total_seconds()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4QulgoC-QOZO"
      },
      "source": [
        "class TweetDataset(IterableDataset):\n",
        "\n",
        "    def __init__(self, fn_data, fn_label, maxlen):\n",
        "        # Store the contents of the file in a pandas dataframe\n",
        "        self.data_reader = open(fn_data, encoding=\"utf-8\")\n",
        "        self.label_dict = json.load(open(fn_label, encoding=\"utf-8\"))\n",
        "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "        # Initialize the BERT tokenizer\n",
        "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "        self.maxlen = maxlen  # the max length of the sentence in the corpus\n",
        "\n",
        "    def __iter__(self):\n",
        "\n",
        "        for line in self.data_reader:\n",
        "            line = line.strip()\n",
        "            if line.endswith(']'):\n",
        "                items = json.loads(line)\n",
        "                id_str = items[0][\"id_str\"]  # Tweet index\n",
        "                idx_list = [pair[0] for pair in create_graph(items).topological_sort()]\n",
        "                tokens_list = []\n",
        "                for idx in idx_list:\n",
        "                    username = items[idx][\"user\"][\"name\"]\n",
        "                    text = items[idx][\"text\"]\n",
        "                    current_sentence = username + \":\" + text\n",
        "                    tokens_list.append(self.tokenizer.tokenize(current_sentence))\n",
        "\n",
        "                tokens_concat = ['[CLS]'] + [token for item in tokens_list for token in item] + ['[SEP]']\n",
        "                if len(tokens_concat) < self.maxlen:\n",
        "                    padded_tokens = tokens_concat + ['[PAD]' for _ in range(self.maxlen - len(tokens_concat))]\n",
        "                else:\n",
        "                    padded_tokens = tokens_concat[:self.maxlen - 1] + ['[SEP]']\n",
        "\n",
        "                tokens_ids = self.tokenizer.convert_tokens_to_ids(padded_tokens)\n",
        "                attn_mask = [1 if token != '[PAD]' else 0 for token in padded_tokens]\n",
        "                tokens_ids_tensor = torch.tensor(tokens_ids)\n",
        "                attn_masks_tensor = torch.tensor(attn_mask)\n",
        "                label = self.label_dict[id_str]  # Tweet label\n",
        "                yield tokens_ids_tensor, attn_masks_tensor, label\n",
        "            else:\n",
        "                raise KeyError(\"Lines are not in format!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BbxrXgpJQPmR"
      },
      "source": [
        "class RumourClassifier(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(RumourClassifier, self).__init__()\n",
        "        # Instantiating BERT model object\n",
        "        self.bert_layer = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "        # Classification layer\n",
        "        # input dimension is 768 because [CLS] embedding has a dimension of 768\n",
        "        # output dimension is 1 because we're working with a binary classification problem\n",
        "        self.cls_layer = nn.Linear(768, 1)\n",
        "\n",
        "    def forward(self, tokens_ids, attn_masks):\n",
        "        '''\n",
        "        Inputs:\n",
        "            -seq : Tensor of shape [B, T] containing token ids of sequences\n",
        "            -attn_masks : Tensor of shape [B, T] containing attention masks to be used to avoid contibution of PAD tokens\n",
        "        '''\n",
        "\n",
        "        # Feeding the input to BERT model to obtain contextualized representations\n",
        "        outputs = self.bert_layer(input_ids=tokens_ids, attention_mask=attn_masks)\n",
        "        cont_reps = outputs.last_hidden_state\n",
        "\n",
        "        # Obtaining the representation of [CLS] head (the first token)\n",
        "        cls_rep = cont_reps[:, 0]\n",
        "\n",
        "        # Feeding cls_rep to the classifier layer\n",
        "        logits = self.cls_layer(cls_rep)\n",
        "\n",
        "        return logits\n",
        "\n",
        "\n",
        "def get_accuracy_from_logits(logits, labels):\n",
        "    probs = torch.sigmoid(logits.unsqueeze(-1))\n",
        "    soft_probs = (probs > 0.5).long()\n",
        "    acc = (soft_probs.squeeze() == labels).float().mean()\n",
        "    return acc\n",
        "\n",
        "\n",
        "def evaluate(net, criterion, dataloader, gpu):\n",
        "    net.eval()\n",
        "\n",
        "    mean_acc, mean_loss = 0, 0\n",
        "    count = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for seq, attn_masks, labels in dataloader:\n",
        "            seq, attn_masks, labels = seq.cuda(gpu), attn_masks.cuda(gpu), labels.cuda(gpu)\n",
        "            logits = net(seq, attn_masks)\n",
        "            mean_loss += criterion(logits.squeeze(-1), labels.float()).item()\n",
        "            mean_acc += get_accuracy_from_logits(logits, labels)\n",
        "            count += 1\n",
        "\n",
        "    return mean_acc / count, mean_loss / count\n",
        "\n",
        "\n",
        "def train(net, criterion, opti, train_loader, dev_loader, max_eps, gpu):\n",
        "    best_acc = 0\n",
        "    st = time.time()\n",
        "    for ep in range(max_eps):\n",
        "\n",
        "        for it, (seq, attn_masks, labels) in enumerate(train_loader):\n",
        "            # Clear gradients\n",
        "            opti.zero_grad()\n",
        "            # Converting these to cuda tensors\n",
        "            seq, attn_masks, labels = seq.cuda(gpu), attn_masks.cuda(gpu), labels.cuda(gpu)\n",
        "\n",
        "            # Obtaining the logits from the model\n",
        "            logits = net(seq, attn_masks)\n",
        "\n",
        "            # Computing loss\n",
        "            loss = criterion(logits.squeeze(-1), labels.float())\n",
        "\n",
        "            # Backpropagating the gradients\n",
        "            loss.backward()\n",
        "\n",
        "            # Optimization step\n",
        "            opti.step()\n",
        "\n",
        "            if it % 100 == 0:\n",
        "                acc = get_accuracy_from_logits(logits, labels)\n",
        "                print(\"Iteration {} of epoch {} complete. Loss: {}; Accuracy: {}; Time taken (s): {}\".format(it, ep,\n",
        "                                                                                                             loss.item(),\n",
        "                                                                                                             acc, (\n",
        "                                                                                                                     time.time() - st)))\n",
        "                st = time.time()\n",
        "\n",
        "        dev_acc, dev_loss = evaluate(net, criterion, dev_loader, gpu)\n",
        "        print(\"Epoch {} complete! Development Accuracy: {}; Development Loss: {}\".format(ep, dev_acc, dev_loss))\n",
        "        if dev_acc > best_acc:\n",
        "            print(\"Best development accuracy improved from {} to {}, saving model...\".format(best_acc, dev_acc))\n",
        "            best_acc = dev_acc\n",
        "            torch.save(net.state_dict(), 'sstcls_{}.dat'.format(ep))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BjKJ36JoQVE2"
      },
      "source": [
        "gpu = 0\n",
        "print(\"Creating the classifier, initialised with pretrained BERT-BASE parameters...\")\n",
        "net = RumourClassifier()\n",
        "net.cuda(gpu)  # Enable gpu support for the model\n",
        "print(\"Done creating the classifier.\")\n",
        "# Define loss function based on binary cross-entropy.\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "opti = optim.Adam(net.parameters(), lr=2e-5)\n",
        "num_epoch = 30\n",
        "\n",
        "train_dataset = TweetDataset(DATA_TRAIN_PATH, LABEL_TRAIN_PATH, maxlen=1800)\n",
        "dev_dataset = TweetDataset(DATA_DEV_PATH, LABEL_DEV_PATH, maxlen=1800)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=50)\n",
        "dev_dataloader = DataLoader(dev_dataset, batch_size=50)\n",
        "\n",
        "train(net, criterion, opti, train_dataloader, dev_dataloader, num_epoch, gpu)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}